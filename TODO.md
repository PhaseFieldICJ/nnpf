- add resulting images to log ?
- CHECK Convolution classes & add `flat_meshgrid`
- tools to visualize dataset
- decorator for `forward` method that detect numpy input, disable grad & reshape? See also reshape in ReactionProblem
- diff, `total_variation_norm`, norm in another module?
- consistant way of specifying domain bounds, kernel size, ... in the command-line (e.g. [0,1]x[0,1], 15x15, maybe event [0,1]Â³ or [0,1]^3 ?!)
- more shapes and operations, fix elongate, express everything from basic shape + op (sphere <=> rounded dot, rectangle <=> elongated dot, ...)
- `batch_size`, `batch_shuffle`, `lr`, `train_N`, `val_N` and dataloader method in Problem?
- dataset generation and loss calculation on multiple steps in a dedicated class (eg EvolutionProblem)?
- 64 bits precision support (see https://github.com/PyTorchLightning/pytorch-lightning/issues/2497)
- adding dtype in every possible class (eg Domain)
- optimization, speed and memory footprint, eg: dataset generated on the fly instead of fully stored, using broadcasting in domain.X and K, lazy expression using Keops, TorchScript, ...
- load checkpoint from GPU to CPU (use `map_location` or always save to cpu?)
- investigate high cpu load (eg on cluster24-math)
- method for saving model (without trainer)
- what about the training and validation dataset when resuming ? (with seed fixed)
- add more tests (model learning, checkpointing, loading, ...)
- AllenCahnSplitting also directly from model (without checkpoint)? (how to save hyperparameters?)
- faster fftconv with padding != 'circular' using `scipy.fftpack.next_fast_len`
- base class for Allen-Cahn and Willmore problem
- training and validation dataset in dedicated class (using LightningDataModule)
- reload hyperparameters when reloading from checkpoint ?
- dedicated module
- remove utils functions from trainer
- epoch=#-v#.ckpt ?! (collide with auto saving initial state in trainer.Trainer)
- common animation script or functions
- animation features for `allen_cahn_problem`
- a hparams.yaml is written in `default_root_dir` ?!
