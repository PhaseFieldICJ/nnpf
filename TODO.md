- add resulting images to log ?
- add loaded checkpoints in hyper-parameters.
- adding hyper-parameters of loaded models in main model hyper-parameters?
- CHECK Convolution classes & add `flat_meshgrid`
- launch script from hyper parameters YAML file
- tools to visualize dataset
- decorator for `forward` method that detect numpy input, disable grad & reshape? See also reshape in ReactionProblem
- diff, `total_variation_norm`, norm in another module?
- consistant way of specifying domain bounds, kernel size, ... in the command-line (e.g. [0,1]x[0,1], 15x15, maybe event [0,1]Â³ or [0,1]^3 ?!)
- replace isabstract in `load_from_checkpoint` by a comparison of class path & name (to allow deriving from a complete model)
- more shapes and operations, fix elongate, express everything from basic shape + op (sphere <=> rounded dot, rectangle <=> elongated dot, ...)
- `batch_size`, `batch_shuffle`, `lr`, `train_N`, `val_N` and dataloader method in Problem?
- dataset generation and loss calculation on multiple steps in a dedicated class (eg EvolutionProblem)?
- 64 bits precision support (see https://github.com/PyTorchLightning/pytorch-lightning/issues/2497)
- adding dtype in every possible class (eg Domain)
- possibility to reload a model from a checkpoint and continue training. With possible different learning rate? With different version name (do track history of the training)?
- check on GPU
- optimization, speed and memory footprint, eg: dataset generated on the fly instead of fully stored, using broadcasting in domain.X and K, lazy expression using Keops, TorchScript, ...
- load checkpoint from GPU to CPU (use `map_location` or always save to cpu?)
- investigate high cpu load (eg on cluster24-math)
- check error with kernel size of 256 and domain size of 256 in HeatArray
- method for saving model (without trainer)
